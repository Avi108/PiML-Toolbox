
.. DO NOT EDIT.
.. THIS FILE WAS AUTOMATICALLY GENERATED BY SPHINX-GALLERY.
.. TO MAKE CHANGES, EDIT THE SOURCE PYTHON FILE:
.. "auto_examples\1_train\plot_2_register_3_arbitrary_pyspark.py"
.. LINE NUMBERS ARE GIVEN BELOW.

.. only:: html

    .. note::
        :class: sphx-glr-download-link-note

        :ref:`Go to the end <sphx_glr_download_auto_examples_1_train_plot_2_register_3_arbitrary_pyspark.py>`
        to download the full example code or to run this example in your browser via Binder

.. rst-class:: sphx-glr-example-title

.. _sphx_glr_auto_examples_1_train_plot_2_register_3_arbitrary_pyspark.py:


Register PySpark Models
========================================================
Here we show how to write a wrapper for PySpark models.

.. GENERATED FROM PYTHON SOURCE LINES 9-10

For demonstration, we first fit a Decision tree model using SimuCredit data.

.. GENERATED FROM PYTHON SOURCE LINES 10-36

.. code-block:: Python

    import os
    os.environ["PYSPARK_PYTHON"] = "python"

    import pandas as pd
    from pyspark.ml import Pipeline
    from pyspark.sql import SparkSession
    from pyspark.ml.classification import DecisionTreeClassifier
    from pyspark.ml.feature import VectorAssembler, StringIndexer

    data = pd.read_csv('https://github.com/SelfExplainML/PiML-Toolbox/blob/main/datasets/SimuCredit.csv?raw=true')
    feature_names = ['Mortgage', 'Balance', 'Amount Past Due', 'Credit Inquiry', 'Open Trade', 'Delinquency', 'Utilization']
    target_name = 'Approved'

    spark = SparkSession.builder.appName("SimuCredit-Spark-Demo").getOrCreate()
    spark_df = spark.createDataFrame(data)

    feature_assembler = VectorAssembler(inputCols=feature_names, outputCol="features")
    label_stringIdx = StringIndexer(inputCol=target_name, outputCol='label')

    pipeline = Pipeline(stages=[feature_assembler, label_stringIdx])
    pipelineModel = pipeline.fit(spark_df)
    train_data, test_data = pipelineModel.transform(spark_df).randomSplit([0.8, 0.2], seed=2024)

    dt = DecisionTreeClassifier(featuresCol='features', labelCol='label', seed=2024)
    model = dt.fit(train_data)








.. GENERATED FROM PYTHON SOURCE LINES 37-38

Next, we define the wrapper functions of predict and predict_proba.

.. GENERATED FROM PYTHON SOURCE LINES 38-52

.. code-block:: Python


    def predict_func(X):

        spark_df = pipelineModel.transform(spark.createDataFrame(pd.DataFrame(X, columns=feature_names)))
        pred = model.transform(spark_df).select('prediction').toPandas().values.astype(float).ravel()
        return pred

    def predict_proba_func(X):
    
        spark_df = pipelineModel.transform(spark.createDataFrame(pd.DataFrame(X, columns=feature_names)))
        predictions = model.transform(spark_df).select('probability').toPandas()
        proba = predictions.explode('probability').values.reshape((-1, 2)).astype(float)
        return proba








.. GENERATED FROM PYTHON SOURCE LINES 53-54

Register the fitted model into PiML (please make sure the datasets of different pipelines are the same)

.. GENERATED FROM PYTHON SOURCE LINES 54-67

.. code-block:: Python

    from piml import Experiment
    exp = Experiment(highcode_only=True)
    pipeline = exp.make_pipeline(predict_func=predict_func,
                                 predict_proba_func=predict_proba_func,
                                 task_type="classification",
                                 train_x=train_data.select(feature_names).toPandas().values,
                                 train_y=train_data.select(target_name).toPandas().values,
                                 test_x=test_data.select(feature_names).toPandas().values,
                                 test_y=test_data.select(target_name).toPandas().values,
                                 feature_names=feature_names,
                                 target_name=target_name)
    exp.register(pipeline, "Spark-DT")








.. GENERATED FROM PYTHON SOURCE LINES 68-69

Check model performance

.. GENERATED FROM PYTHON SOURCE LINES 69-71

.. code-block:: Python

    exp.model_diagnose(model="Spark-DT", show="accuracy_table")





.. rst-class:: sphx-glr-script-out

 .. code-block:: none

               ACC      AUC       F1  LogLoss    Brier
                                                  
    Train   0.3106   0.2718   0.2642   0.9848   0.3801
    Test    0.3085   0.2706   0.2585   0.9839   0.3798
    Gap    -0.0020  -0.0011  -0.0057  -0.0009  -0.0003




.. GENERATED FROM PYTHON SOURCE LINES 72-73

Run validataion tests

.. GENERATED FROM PYTHON SOURCE LINES 73-74

.. code-block:: Python

    exp.model_explain(model="Spark-DT", show="pdp", uni_feature="Balance", sample_size=1000, figsize=(5, 4))



.. image-sg:: /auto_examples/1_train/images/sphx_glr_plot_2_register_3_arbitrary_pyspark_001.png
   :alt: Partial Dependence Plot
   :srcset: /auto_examples/1_train/images/sphx_glr_plot_2_register_3_arbitrary_pyspark_001.png
   :class: sphx-glr-single-img






.. rst-class:: sphx-glr-timing

   **Total running time of the script:** (12 minutes 24.583 seconds)


.. _sphx_glr_download_auto_examples_1_train_plot_2_register_3_arbitrary_pyspark.py:

.. only:: html

  .. container:: sphx-glr-footer sphx-glr-footer-example

    .. container:: binder-badge

      .. image:: images/binder_badge_logo.svg
        :target: https://mybinder.org/v2/gh/selfexplainml/piml-toolbox/main?urlpath=lab/tree/./docs/_build/html/notebooks/auto_examples/1_train/plot_2_register_3_arbitrary_pyspark.ipynb
        :alt: Launch binder
        :width: 150 px

    .. container:: sphx-glr-download sphx-glr-download-jupyter

      :download:`Download Jupyter notebook: plot_2_register_3_arbitrary_pyspark.ipynb <plot_2_register_3_arbitrary_pyspark.ipynb>`

    .. container:: sphx-glr-download sphx-glr-download-python

      :download:`Download Python source code: plot_2_register_3_arbitrary_pyspark.py <plot_2_register_3_arbitrary_pyspark.py>`


.. only:: html

 .. rst-class:: sphx-glr-signature

    `Gallery generated by Sphinx-Gallery <https://sphinx-gallery.github.io>`_
