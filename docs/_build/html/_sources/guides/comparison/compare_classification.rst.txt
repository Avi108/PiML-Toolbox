.. Places parent toc into the sidebar

:parenttoc: True

.. include:: ../../includes/big_toc_css.rst

================================
Comparison for Classification
================================
This section compares models from various perspectives. This can be done by using the `model_compare` function in PiML. For demonstration, we consider a regression task on the BikeSharing data.



Accuracy Comparison
^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
We consider a binary classification task on the Taiwan Credit. The `metric` can be chosen from "ACC", "AUC", and "F1".

Accuracy Score
""""""""""""""""""""""""""
The bar chart below compares models based on ACC score, with `metric` = "ACC". As the legend indicates, the plot compares the models on training and testing data. The results indicate that these three models have similar performance.

.. jupyter-input::

   exp.model_compare(models=["Tree", "FIGS", "XGB2"], show="accuracy_plot", metric="ACC")

.. figure:: ../../auto_examples/testing_compare/images/sphx_glr_plot_0_compare_classification_001.png
   :target: ../../auto_examples/testing_compare/plot_0_compare_classification.html
   :align: left


AUC Score
""""""""""""""""""""""""""
The second plot compares models based on AUC score, with `metric` = "AUC". The results indicate that the XGB2 model has slightly better performance.

.. jupyter-input::

   exp.model_compare(models=["Tree", "FIGS", "XGB2"], show="accuracy_plot", metric="AUC")
      
.. figure:: ../../auto_examples/testing_compare/images/sphx_glr_plot_0_compare_classification_002.png
   :target: ../../auto_examples/testing_compare/plot_0_compare_classification.html
   :align: left


F1 Score
""""""""""""""""""""""""""
The last plot compares modes based on F1 score, with `metric` = "F1". All these three models have similar performance under this metric.

.. jupyter-input::

   exp.model_compare(models=["Tree", "FIGS", "XGB2"], show="accuracy_plot", metric="F1")
      
.. figure:: ../../auto_examples/testing_compare/images/sphx_glr_plot_0_compare_classification_003.png
   :target: ../../auto_examples/testing_compare/plot_0_compare_classification.html
   :align: left



Overfit Comparison
^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
Overfit comparison is to compare the overfit regions of different models. The overfit detection algorithm can be found in the overfit_ testing section. The keyword for overfit comparison is "overfit". Similar to the overfit test for a single model, the following arguments are required to run the overfit comparison. 

.. _overfit: ../testing/overfit.html

Different from the overfit for a single model, the argument for selecting slicing features is `slice_feature` (instead of `slice_features`), which is a string representing the feature name. The following example illustrates how to compare models' overfit regions, using the keyword "overfit", in which we consider a binary classification task on the TaiwanCredit data for demonstration.

.. jupyter-input::

   exp.model_compare(models=["Tree", "FIGS", "XGB2"], show="overfit",
                     slice_method="historgram", slice_feature="PAY_1", threshold=1.05, 
                     bins=10, metricmetric="ACC", figsize=(5, 4))

.. figure:: ../../auto_examples/testing_compare/images/sphx_glr_plot_0_compare_classification_004.png
   :target: ../../auto_examples/testing_compare/plot_0_compare_classification.html
   :align: left


.. jupyter-input::

   exp.model_compare(models=["Tree", "FIGS", "XGB2"], show="overfit",
                     slice_method="tree", slice_feature="PAY_1", threshold=1.05, 
                     metricmetric="ACC", figsize=(5, 4))

.. figure:: ../../auto_examples/testing_compare/images/sphx_glr_plot_0_compare_classification_005.png
   :target: ../../auto_examples/testing_compare/plot_0_compare_classification.html
   :align: left



Reliability Comparison
^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
For binary classification tasks, the reliability comparison includes bandwidth comparison and reliability diagram comparison.


Bandwidth Comparison
""""""""""""""""""""""""""""""
The prediction interval bandwidth for binary classfication is quantified by the squared root of :math:`\hat{p}(1 - \hat{p})`, and hence the argument `alpha` is not used. The example below illustrates how to compare models' prediciton invertal bandwidth. 

.. jupyter-input::

   exp.model_compare(models=["Tree", "FIGS", "XGB2"], show="reliability_bandwidth", figsize=(5, 4))
                  
.. figure:: ../../auto_examples/testing_compare/images/sphx_glr_plot_0_compare_classification_006.png
   :target: ../../auto_examples/testing_compare/plot_0_compare_classification.html
   :align: left


Reliability Diagram Comparison
""""""""""""""""""""""""""""""
The keyword for reliability diagram is "reliability_perf", and it generates a reliability diagram for each model, as shown below. The argument `bins` is the number of bins of predicted probability, used for calculating the calibrated probability using the actual label.

.. jupyter-input::

   exp.model_compare(models=["Tree", "FIGS", "XGB2"], show="reliability_perf",
                     bins=10, figsize=(5, 4))
                  
.. figure:: ../../auto_examples/testing_compare/images/sphx_glr_plot_0_compare_classification_007.png
   :target: ../../auto_examples/testing_compare/plot_0_compare_classification.html
   :align: left



Robustness Comparison
^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
Robustness comparison is to compare models under input perturbation. This section illustrates how to compare models under input perturbation. The algorithm details of robustness test can be found in the robustness_ testing section.

.. _robustness: ../testing/robustness.html


Robustness Perforformance
""""""""""""""""""""""""""""""
The example below illustrates how to compare models' robustness performance using the keyword "robustness_perf". The perturbation method is set to "raw", which means we add normal noise on the raw data of numerical features. The perturbation step size is set to 0.2. By default, the perturbation features are all features. The performance metric is set to "AUC". The following example illustrates how to compare models' robustness performance.

.. jupyter-input::

   exp.model_compare(models=["Tree", "FIGS", "XGB2"], show="robustness_perf",
                     perturb_method="raw", perturb_size=0.2, metric="AUC", figsize=(5, 4))

.. figure:: ../../auto_examples/testing_compare/images/sphx_glr_plot_0_compare_classification_008.png
   :target: ../../auto_examples/testing_compare/plot_0_compare_classification.html
   :align: left

In the plot above, the perturbation is applied on all variables. On the x-axis, we have the perturbation size, and on the y-axis, the model performance. Model XGB2 recorded the best robustness performance, followed by XGB7. The worst performing is GLM. 


Robustness Performance on Worst Samples
"""""""""""""""""""""""""""""""""""""""""""
The keyword "robustness_perf_worst" is used to evaluate the models' performance against perturbation size based on the worst sample. For this option, in addition to the arguments `metric`, `perturb_method`, `perturb_size`, and `perturb_features`, we have an additional argument `alpha`, which is the proportion of worst samples to consider. The following example illustrates how to compare models' robustness performance on worst samples.

.. jupyter-input::

   exp.model_compare(models=["Tree", "FIGS", "XGB2"], show="robustness_perf_worst",
                     perturb_method="quantile", perturb_size=0.1, metric="F1", alpha=0.3, figsize=(5, 4))
                  
.. figure:: ../../auto_examples/testing_compare/images/sphx_glr_plot_0_compare_classification_009.png
   :target: ../../auto_examples/testing_compare/plot_0_compare_classification.html
   :align: left

The plot shows that XGB2 performs well on the unperturbed data (noise = 0) and it is also the most robust model against perturbation. GAM is the second best model, followed by GLM. 



Resilience Comparison
^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
Resilience comparison is to compare models under input perturbation. This section illustrates how to compare models under input perturbation. The algorithm details of resilience test can be found in the resilience_ testing section.

.. _resilience: ../testing/resilience.html


Resilience Perforformance
""""""""""""""""""""""""""""""
The example below illustrates how to compare models' resilience performance using the keyword "resilience_perf". The perturbation method is set to "worst-sample". The performance metric is set to "AUC". The following example illustrates how to compare models' resilience performance.

.. jupyter-input::

   exp.model_compare(models=["Tree", "FIGS", "XGB2"], show="resilience_perf",
                     resilience_method="worst-sample", immu_feature=None, metric="AUC", figsize=(5, 4))

.. figure:: ../../auto_examples/testing_compare/images/sphx_glr_plot_0_compare_classification_010.png
   :target: ../../auto_examples/testing_compare/plot_0_compare_classification.html
   :align: left




Resilience Performance on Worst Samples
"""""""""""""""""""""""""""""""""""""""""""
The keyword `resilience_perf_worst` is used to evaluate the models' resilience performance on the worst sample. For this option, in addition to the arguments `resilience_method`, `immu_feature`, and `metric`, we have an additional argument `alpha`, which is the proportion of worst samples to consider, as `resilience_method` is "worst-sample", "hard-sample", or "outer-sample"; or `n_clusters` the value of :math:`K` in Kmean clustering as `resilience_method` is "worst-cluster". The following example illustrates how to compare models' robustness performance on worst samples.

.. jupyter-input::

   exp.model_compare(models=["Tree", "FIGS", "XGB2"], show="resilience_perf_worst",
                     resilience_method="worst-sample", immu_feature=None, metric="AUC",
                     alpha=0.3, figsize=(5, 4))
                  
.. figure:: ../../auto_examples/testing_compare/images/sphx_glr_plot_0_compare_classification_011.png
   :target: ../../auto_examples/testing_compare/plot_0_compare_classification.html
   :align: left



Examples
^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^

.. topic:: Examples 1: Taiwan Credit

  The second example below demonstrates how to use PiML’s high-code APIs for the TaiwanCredit dataset from the UCI repository. This dataset comprises the credit card details of 30,000 clients in Taiwan from April 2005 to September 2005, and more information can be found on the TaiwanCreditData website. The data can be loaded directly into PiML, although it requires some preprocessing. The FlagDefault variable serves as the response for this classification problem.

 * :ref:`sphx_glr_auto_examples_testing_compare_plot_0_compare_regression.py`
