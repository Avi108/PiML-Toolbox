.. Places parent toc into the sidebar

:parenttoc: True

.. include:: ../../includes/big_toc_css.rst

========================================
Comparison for Regression
========================================
This section describes how different regression models for continuous responses can be compared on the basis of different metrics. In PiML, this can be done by using the `model_compare` function. We use results from the BikeSharing data for illustration.



Accuracy Comparison
^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
The metric can be chosen to be one of "MSE", "MAE",  or "R2". Comparisons are shown on "accuracy_plot".

Mean Squared Error
""""""""""""""""""""""""""
The first plotFirst, we compare models based on the squared error of the observations on training and test data: :math:`SE = (y_i-\hat{y}_i)^2`. This is done with metric set to "MSE" in the function. The plot displays boxplots for SE for both testing and training data. In addition, the mean squared error (MSE) is marked using the a circle.

.. jupyter-input::

      exp.model_compare(models=["GLM", "XGB2", "XGB7"], show="accuracy_plot",
                        metric="MSE", figsize=(5, 4))

.. figure:: ../../auto_examples/testing_compare/images/sphx_glr_plot_0_compare_regression_001.png
   :target: ../../auto_examples/testing_compare/plot_0_compare_regression.html
   :align: left

Mean Absolute Error
""""""""""""""""""""""""""
In this case, we compare the models based on the absolute error :math:`|y_i-\hat{y}_i|`. The PiML code should have `metric` set to "MAE".

.. jupyter-input::

      exp.model_compare(models=["GLM", "XGB2", "XGB7"], show="accuracy_plot",
                        metric="MAE", figsize=(5, 4))
      
.. figure:: ../../auto_examples/testing_compare/images/sphx_glr_plot_0_compare_regression_002.png
   :target: ../../auto_examples/testing_compare/plot_0_compare_regression.html
   :align: left

The relative comparisons of the three models results are the same as before.

R-squared Score
""""""""""""""""""""""""""
The third comparison is in terms of the R2 score.

.. jupyter-input::

      exp.model_compare(models=["GLM", "XGB2", "XGB7"], show="accuracy_plot",
                        metric="R2", figsize=(5, 4))
      
.. figure:: ../../auto_examples/testing_compare/images/sphx_glr_plot_0_compare_regression_003.png
   :target: ../../auto_examples/testing_compare/plot_0_compare_regression.html
   :align: left

The bar chart above compares models based on R2. Note that there is only a single R2 score for the entire dataset, so there are no box plots. The conclusions from R2 is again the same as SE and AE.


Overfit Comparison
^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
Overfit comparison displays the overfit regions of different models against a feature of interest. The detection algorithm can be found in the overfit_ section. Different from the overfit for a single model, the argument for selecting slicing features is `slice_feature` (instead of `slice_features`), which is a string representing the feature name. The following example illustrates how to compare models' overfit regions, using the keyword "overfit", in which we consider a regression task on the BikeSharing data.

.. _overfit: ../testing/overfit.html

.. jupyter-input::

   exp.model_compare(models=["GLM", "XGB2", "XGB7"], show="overfit",
                     slice_method="histogram", slice_feature="hr", threshold=1.05, 
                     bins=10, metricmetric="MSE", figsize=(5, 4))

.. figure:: ../../auto_examples/testing_compare/images/sphx_glr_plot_0_compare_regression_004.png
   :target: ../../auto_examples/testing_compare/plot_0_compare_regression.html
   :align: left

.. jupyter-input::

   exp.model_compare(models=["GLM", "XGB2", "XGB7"], show="overfit",
                     slice_method="tree", slice_feature="atemp", threshold=1.05, 
                     metricmetric="ACC", figsize=(5, 4))

.. figure:: ../../auto_examples/testing_compare/images/sphx_glr_plot_0_compare_regression_005.png
   :target: ../../auto_examples/testing_compare/plot_0_compare_regression.html
   :align: left


Reliability Comparison
^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
Reliability comparison is to compare models under input perturbation. The algorithm details of the robustness test can be found in the reliability_ testing section.

.. _reliability: ../testing/reliability.html

Coverage Comparison
""""""""""""""""""""""""""""""""
For demonstration purposes, we consider 3 models on the BikeSharing data. By setting `show` to "reliability_coverage", we can get the average coverage comparison plot of the prediction intervals on the test set. The argument `alpha` is the expected proportion of test samples to be outside the prediction intervals. 

.. jupyter-input::

   exp.model_compare(models=["GLM", "XGB2", "XGB7"], show="reliability_coverage",
                     alpha=0.1, figsize=(5, 4))

.. figure:: ../../auto_examples/testing_compare/images/sphx_glr_plot_0_compare_regression_006.png
   :target: ../../auto_examples/testing_compare/plot_0_compare_regression.html
   :align: left

Bandwidth Comparison
""""""""""""""""""""""""""""""
The example below illustrates how to compare models' prediction interval bandwidth. The argument `alpha` is set to 0.1, which means the expected coverage is 90%.

.. jupyter-input::

   exp.model_compare(models=["GLM", "XGB2", "XGB7"], show="reliability_bandwidth",
                     alpha=0.1, figsize=(5, 4))

.. figure:: ../../auto_examples/testing_compare/images/sphx_glr_plot_0_compare_regression_007.png
   :target: ../../auto_examples/testing_compare/plot_0_compare_regression.html
   :align: left


Robustness Comparison
^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
Robustness comparison is to compare models under input perturbation. This section illustrates how to compare models under input perturbation. The algorithm details of the robustness test can be found in the robustness_ testing section.

.. _robustness: ../testing/robustness.html

Robustness Performance
""""""""""""""""""""""""""""""
The example below illustrates how to compare models' robustness performance using the keyword "robustness_perf". The perturbation method is set to "raw", which means we add normal noise to the raw data of numerical features. The perturbation step size is set to 0.2. By default, the perturbation features are all features. The performance metric is set to "AUC". The following example illustrates how to compare models' robustness performance.

.. jupyter-input::

   exp.model_compare(models=["GLM", "XGB2", "XGB7"], show="robustness_perf",
                     perturb_method="raw", perturb_size=0.2, metric="MSE", figsize=(5, 4))

.. figure:: ../../auto_examples/testing_compare/images/sphx_glr_plot_0_compare_regression_008.png
   :target: ../../auto_examples/testing_compare/plot_0_compare_regression.html
   :align: left

In the plot above, the perturbation is applied to all variables. On the x-axis, we have the perturbation size, and on the y-axis, the model performance. Model XGB2 recorded the best robustness performance, followed by XGB7. The worst performing is GLM. 

Robustness Performance on Worst Samples
"""""""""""""""""""""""""""""""""""""""""""
The keyword "robustness_perf_worst" is used to evaluate the models' performance against perturbation size based on the worst sample. For this option, in addition to the arguments `metric`, `perturb_method`, `perturb_size`, and `perturb_features`, we have an additional argument `alpha`, which is the proportion of worst samples to consider. The following example illustrates how to compare models' robustness performance on worst samples.

.. jupyter-input::

   exp.model_compare(models=["GLM", "XGB2", "XGB7"], show="robustness_perf_worst",
                     perturb_method="quantile", perturb_size=0.1, metric="R2", alpha=0.3, figsize=(5, 4))
                  
.. figure:: ../../auto_examples/testing_compare/images/sphx_glr_plot_0_compare_regression_009.png
   :target: ../../auto_examples/testing_compare/plot_0_compare_regression.html
   :align: left

The plot shows that XGB2 performs well on unperturbed data (noise = 0) and it is also the most robust model against perturbation. GAM is the second-best model, followed by GLM. 


Resilience Comparison
^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
Resilience comparison is to compare models under input perturbation. This section illustrates how to compare models under input perturbation. The algorithm details of the resilience test can be found in the resilience_ testing section.

.. _resilience: ../testing/resilience.html

Resilience Performance
""""""""""""""""""""""""""""""
The example below illustrates how to compare models' resilience performance using the keyword "resilience_perf". The perturbation method is set to "worst-sample". The performance metric is set to "AUC". The following example illustrates how to compare models' resilience performance.

.. jupyter-input::

   exp.model_compare(models=["GLM", "XGB2", "XGB7"], show="resilience_perf",
                     resilience_method="worst-sample", immu_feature=None, metric="MSE", figsize=(5, 4))

.. figure:: ../../auto_examples/testing_compare/images/sphx_glr_plot_0_compare_regression_010.png
   :target: ../../auto_examples/testing_compare/plot_0_compare_regression.html
   :align: left

Resilience Performance on Worst Samples
"""""""""""""""""""""""""""""""""""""""""""
The keyword `resilience_perf_worst` is used to evaluate the models' resilience performance on the worst sample. For this option, in addition to the arguments `resilience_method`, `immu_feature`, and `metric`, we have an additional argument `alpha`, which is the proportion of worst samples to consider, as `resilience_method` is "worst-sample", "hard-sample", or "outer-sample"; or `n_clusters` the value of :math:`K` in Kmean clustering as `resilience_method` is "worst-cluster". The following example illustrates how to compare models' robustness performance on worst samples.

.. jupyter-input::

   exp.model_compare(models=["GLM", "XGB2", "XGB7"], show="resilience_perf_worst",
                     resilience_method="worst-sample", immu_feature=None, metric="MSE",
                     alpha=0.3, figsize=(5, 4))
                  
.. figure:: ../../auto_examples/testing_compare/images/sphx_glr_plot_0_compare_regression_011.png
   :target: ../../auto_examples/testing_compare/plot_0_compare_regression.html
   :align: left


Examples
^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
                        
.. topic:: Example 1: BikeSharing

  The first example below demonstrates how to use PiML with its high-code APIs for developing machine learning models for the BikeSharing data from the UCI repository, which consists of 17,389 samples of hourly counts of rental bikes in Capital bikeshare system; see details. The response `cnt` (hourly bike rental counts) is continuous and it is a regression problem.
 
 * :ref:`sphx_glr_auto_examples_testing_compare_plot_0_compare_regression.py`
