.. Places parent toc into the sidebar

:parenttoc: True

.. include:: ../../includes/big_toc_css.rst

============================
Fairness Comparison
============================
This section deals with the module for assessing/comparing bias and fairness of models. We use the "SimuCredit" dataset and illustrate the results on two models, GLMClassifier (GLM) and ExplainableBoostingClassifier (EBM). The model performance of EBM is better than GLM because the former is a more complex machine learning model. But we will see a different view based on fairness comparison.



Fairness Metrics
^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
In fairness comparison, each fairness testing result of the single model is from the fairness test module. See the definition_ of each fairness metric.

.. _definition: ../testing/fairness.html#fairness-metric

Below, we compare the fairness of EBM against GLM on the (simulated) credit scoring dataset. The comparisons are made on two different protected attributes: race and gender. Note that such protected attributes cannot be directly used in the model. So any differences in fairness is likely due to the effects of surrogate variables that are related to the protected attributes. The PiML code for comparison is shown below. 

.. jupyter-input::

   metrics_result = exp.model_fairness_compare(models=["GLM", "EBM"], show="metrics", metric="AIR",
                                               group_category=["Race", "Gender"],
                                               reference_group=[1., 1.], protected_group=[0., 0.],
                                               favorable_threshold=0.5,
                                               figsize=(6, 4), return_data=True)

.. figure:: ../../auto_examples/testing_compare/images/sphx_glr_plot_1_compare_fairness_001.png
   :target: ../../auto_examples/testing_compare/plot_1_compare_fairness.html
   :align: center

This plot shows the AIR comparisons for two protected attributes. The bar plots on the left show AIRs for Race (African Americans) against whites (not shown) for the two models. The ones on the right are for Gender (women) against men (not shown). The y-axis shows the AIR values. A value of 1 indicates no discrimination. In practice, any value of AIR less than 0.8 is considered discriminative. In the above figure, the AIR values for African Americans and Women are less than 1 for both models, indicating they both discriminate. In particular, the values for EBM model are lower than those for GLM.



Segmented
^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
In addition to an overall comparison based on the entire dataset, we can also compare the models within appropriately segmented subsets of the dataset. The results below provide comparisons within segments of the predictor "Balance".

.. jupyter-input::
   
    segmented_result = exp.model_fairness_compare(models=["GLM", "EBM"], show="segmented", metric="AIR",
                                                  segmented_feature="Balance",
                                                  group_category=["Race", "Gender"],
                                                  reference_group=[1., 1.], protected_group=[0., 0.],
                                                  segmented_bins=5, favorable_threshold=0.5,
                                                  return_data=True, figsize=(8, 4))

.. figure:: ../../auto_examples/testing_compare/images/sphx_glr_plot_1_compare_fairness_002.png
   :target: ../../auto_examples/testing_compare/plot_1_compare_fairness.html
   :align: center

From the plots, we can see the AIR differences between the two models is especially large for Race â€“ American African applicants with low balance. That means the EBM model is more unfair in this region. The relative performances change as balance increases. A similar conclusion holds for Gender (women).



Examples
^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^

The full example codes of this section can be found in the following link.

.. topic:: Example

	* :ref:`sphx_glr_auto_examples_testing_compare_plot_1_compare_fairness.py`

