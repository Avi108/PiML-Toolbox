.. Places parent toc into the sidebar

:parenttoc: True

.. include:: ../../includes/big_toc_css.rst

==============
Decision Tree
==============

The decision tree is a well-known supervised machine learning algorithm that operates by recursively dividing the dataset into smaller subsets based on the most influential features that can accurately predict the response. As the algorithm splits the data, it forms a tree diagram, rendering it highly interpretable.



Model Training
^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^

In this section, we will discuss the process of training a decision tree model in PiML. Assuming that the data has been prepared, the next step is to import the appropriate module, which is either `PiDecisionTreeRegressor` for regression tasks or `PiDecisionTreeClassifier` for binary classification tasks. Once the appropriate module is imported, the next step is to use the `exp.model_train` method to fit the model to the data.

.. jupyter-input::

    from piml.models import PiDecisionTreeClassifier
    exp.model_train(model=PiDecisionTreeClassifier(max_depth=6), name='Tree')

Note that `PiDecisionTreeRegressor` and `PiDecisionTreeClassifier` are essentially wrappers for the `sklearn.tree.DecisionTreeRegressor`_ and `sklearn.tree.DecisionTreeClassifier`_ in sklearn, we offer additional interpretation functionalities. When using decision trees, `max_depth` is often considered the most crucial hyperparameter.

- `max_depth`: an integer limiting the max depth of the tree, by default 5. 
  The split would stop if the tree reaches the max depth constraint. This parameter is commonly used for controlling the model complexity. Shallow trees are easy to interpret, but their predictive power might be limited. Deep trees in general would have better predictive performance, but can also suffer from overfitting and low model interpretability.

.. _sklearn.tree.DecisionTreeRegressor: https://scikit-learn.org/stable/modules/generated/sklearn.tree.DecisionTreeRegressor.html

.. _sklearn.tree.DecisionTreeClassifier: https://scikit-learn.org/stable/modules/generated/sklearn.tree.DecisionTreeClassifier.html


Global Interpretation
^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^

By setting `show` to ``tree_global``, you will see the fitted decision tree diagram.

.. jupyter-input::
     
     exp.model_interpret(model='Tree', show="tree_global", root=0,
                         depth=3, original_scale=True, figsize=(20, 10))

.. figure:: ../../auto_examples/models/images/sphx_glr_plot_2_dt_cls_001.png
   :target: ../../auto_examples/models/plot_2_cls_dt.html
   :align: left
   :scale: 80%

To ensure the tree diagram is useful for deep trees, we offer two parameters that enable you to adjust the view. The first is `root`, which is the node ID where the tree diagram begins. By default, it starts at the actual root node, which is assigned an ID of 0. The second parameter is `depth`, which sets the maximum depth of the diagram starting from the `root` node.

Each box in the diagram represents a node in the decision tree. It includes information such as the node ID, the splitting rule used to split the dataset, the criterion value, the sample size, and the average value of the response.



Local Interpretation
^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^

When using the `tree_Local` keyword, the decision path of a specific sample is highlighted within the tree diagram.

.. jupyter-input::

    exp.model_interpret(model='Tree', show="tree_local", sample_id=0, original_scale=True, figsize=(20, 6))

.. figure:: ../../auto_examples/models/images/sphx_glr_plot_2_dt_cls_003.png
   :target: ../../auto_examples/models/plot_2_dt_cls.html
   :align: left
   :scale: 80%

Note that the plot generated using `tree_local` is a subset of the global tree diagram. Only the branches relevant to the selected sample are shown in the plot, making it easier to interpret the decision path of that particular sample within the context of the entire decision tree.


Examples
^^^^^^^^^^^^^^^^^^^^^

.. topic:: Example 1: CoCircles

  The example below demonstrates how to use PiML with the CoCircles data, which is a simulated binary classification data with two features.

 * :ref:`sphx_glr_auto_examples_models_plot_2_dt_cls.py`

.. _here: https://scikit-learn.org/stable/modules/generated/sklearn.datasets.fetch_california_housing.html
