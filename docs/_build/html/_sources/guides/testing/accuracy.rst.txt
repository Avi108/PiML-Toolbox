.. Places parent toc into the sidebar

:parenttoc: True

.. include:: ../includes/big_toc_css.rst

============================    
Accuracy 
============================
    
This section demonstrates how to use PiML to evaluate our model using key performance metrics such as MSE and MAE for regression tasks and ACC, AUC, Recall, Precision, and F1-score for binary 
classification tasks.

Key terms and definitions
^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^

**Mean Squared Error (MSE)**: Let :math:`n` represents the number of samples, :math:`y` and :math:`\hat{y}` the actual and the predicted value, 
respectively.  The mean square error is computed as 

.. math::
   \begin{align}
       MSE = \frac{1}{n}\sum_{i-1}^{n}(y_{i} - \hat{y}_{i})^{2} \tag{1}
   \end{align}

**True positives (TP)**: The model predicts that an observation belongs to a specific class and it does belong to that class.

**True negatives (TN)**: The model predicts that an observation does not belong to a specific class when it actually does not belong to that class.

**False positives (FP)**: The model predicts that an observation belongs to a specific class when in reality, it does not.

**False negatives (FN)**: The model predicts that an observation does not belong to a specific class when it actually belongs to that class.

**Accuracy**: The accuracy is the number of samples correctly classified divided by the total number of samples. It is commonly used to evaluate classifiers models. However, it is not a better measurement 
when dealing with class-imbalance data. In addition to the accuracy, to fully evaluate the effectiveness of our model, we must look at both precision and recall. 

.. math::
   \begin{align}
      Accuracy = \frac{TP+TN}{TP+FP+TN+FN} \tag{2}
   \end{align}

**Precision**: 

.. math::
   \begin{align}
      Precision = \frac{TP}{TP+FP} \tag{3}
   \end{align}

**Recall**:

.. math::
   \begin{align}
      Recall = \frac{TP}{TP+FN} \tag{4}
   \end{align}

**F1-score**

.. math::
   \begin{align}
      F1-score = 2\frac{Precision \cdot Recall}{Precision+Recall}=\frac{2.TP}{2.TP+FP+FN} \tag{5}
   \end{align}

Precision-Recall is a useful measure of prediction success when the classes are very imbalanced. In information retrieval, precision is a measure of result relevancy, while recall is a 
measure of how many truly relevant results are returned. In practice, if a model have high recall but low precision, most of its predicted labels are not correct when compared to the training labels. On the other 
hand, a model with high precision but low recall will have most of its predicted labels correct when compared to the training labels.
    
Binary classification tasks.
^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^

We consider a binary classification task on the  TaiwanCredit data from UCI repository, which consists of 30,000 clients' credit cards in Taiwan from 200504 to 200509; see details here 
`TaiwanCreditData`_ . The data can be loaded from PiML and subject to slight preprocessing. The response to this classification problem is 'FlagDefault'.

Let's start by training two separate models; we then demonstrate how to use PiML to asses individual performance or compare the modes using a single plot (This approach can be extended to 
multiple models). 
    
.. _TaiwanCreditData: https://archive.ics.uci.edu/ml/datasets/default+of+credit+card+clients

Model Training   
""""""""""""""""""""""""""""""""""""""""""

.. jupyter-input::
 
        import ipywidgets
        with ipywidgets.Output():
          from piml import Experiment
          from piml.models import FIGSClassifier
          from piml.models import ReluDNNClassifier
          from piml.models import GAMINetClassifier
          exp = Experiment()
          exp.data_loader(data='TaiwanCredit')
          exp.data_summary(feature_exclude=["LIMIT_BAL", "SEX", "EDUCATION", "MARRIAGE", "AGE"], feature_type={})
          exp.data_prepare(target='FlagDefault', task_type='Classification', test_ratio=0.2, random_state=0)

.. jupyter-input::    

     exp.model_train(model=ReluDNNClassifier(hidden_layer_sizes=(40, 40), l1_reg=0.0002, learning_rate=0.001),
                                      name='ReLUDNN')
     exp.model_train(model=FIGSClassifier(max_iter=200, max_depth=None, splitter='best', min_samples_leaf=1,
                     min_impurity_decrease=0, learning_rate=0.002, random_state=None), name='FIGS') 
     exp.model_train(GAMINetClassifier(interact_num=10, loss_threshold=0.01,
                     subnet_size_main_effect=[20], subnet_size_interaction=[20,20]),
                     name='GAMINet')

Model performance.
"""""""""""""""""""""""

We first consider the performance of individual models. In addition, we can also compare the performance of many models at the same time (in this demo, we consider three models). Which helps 
identify a model that is suitable for a particular task. 

.. jupyter-input::

        exp.model_diagnose(model="GAMINet", show='accuracy_result')

Compare models' performance

.. jupyter-input::
   
        exp.model_compare(models=["FIGS","ReLUDNN","GAMINet"], show='accuracy_acc', figsize=(5,4))
   
The plot above indicates that `ReluDNN` and `GAMINet` recorded a comparative performance. However, we should consider other factors when selecting our best-performing model. Below is an example of comparing models 
based on the AUC plot.

.. jupyter-input::
          
        exp.model_compare(models=["FIGS","ReLUDNN","GAMINet"], show='accuracy_auc', figsize=(5,4))
          
Regression tasks.
^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^

For a regression task, please refer to the example below.
 
Examples
^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^

The example below demonstrates how to use PiML with its high-code APIs for developing machine learning models for the BikeSharing data from UCI repository, which consists of 17,389
samples of hourly counts of rental bikes in Capital bikeshare system; see details `here`_. The response `cnt` (hourly bike rental counts) is continuous and it is a regression problem.
    
.. _here: https://archive.ics.uci.edu/ml/datasets/bike+sharing+dataset
    
.. topic:: Example
    
 * :ref:`sphx_glr_auto_examples_diagnostics_plot_1_accuracy.py`
    
    


