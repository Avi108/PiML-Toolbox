.. Places parent toc into the sidebar

:parenttoc: True

.. include:: ../../includes/big_toc_css.rst


Fairness Test
======================================

Fairness is the model's quality or state of being fair or impartial. 
Decisions made by computers after a machine-learning process may be considered unfair if they were based on variables considered sensitive. 
Examples of these kinds of variables include gender, ethnicity, sexual orientation, disability and more.


In this section, the model fairness test will be introduced. 
For demonstration purposes, we run the following example codes to initialize a PiML experiment for the Credit Simulation dataset.
In the model training process, we cannot use demographic features like 'Race' and 'Gender'. So we need to remove them from training data.
We use monotonic depth 2 xgboost to train the data.

.. code-block:: default

	>>> from piml import Experiment
	>>> exp = Experiment()
	>>> data = pd.read_csv('https://github.com/SelfExplainML/PiML-Toolbox/blob/main/datasets/CreditSimuBalanced.csv?raw=true')
	>>> exp.data_loader(data)
	>>> exp.data_summary(feature_exclude=['Race', 'Gender'])
	>>> exp.data_prepare(target='Status', task_type='Classification')
	>>> xgb2_mono = PiXGBClassifier(max_depth=2, n_estimators=100, feature_names=exp.get_feature_names(), 
                                    mono_increasing_list=['Mortgage', 'Balance'], 
                                    mono_decreasing_list=['Amount Past Due', 'Utilization', 'Delinquency Status', 'Credit Inquiry', 'Open Trade'])
	>>> xgb2_mono.fit(train_x, train_y)
	>>> xgb2_wrapper = exp.make_pipeline(xgb2_mono)
	>>> exp.register(xgb2_wrapper, name='XGB2_monotonic')


Fairness Metric
^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^

PIML support showing the fairness result for different metrics.

Metric types:

- "AIR": Adverse impact ratio.

.. math::

   \begin{align}     
   AIR = \frac{(TP_{p}+FN_{p}) / n_{r}}{(TP_{r}+FN_{r}) / n_{p}}
   \end{align}

The value of :math:`TP`, :math:`FN`, :math:`n` refers to the count of True Positive samples, False negative samples and all samples,
respectively. The :math:`p` and :math:`r` refers to the group samples belong to.

- "SMD": Standardized mean difference. (Usually used for continuous response)

.. math::

   \begin{align}     
   SMD = \frac{(\sum_{x\in{p}} outcome_x / {n_{p}}) - (\sum_{x\in{r}} outcome_x / n_{r})} {\sigma(outcome)} * 100
   \end{align}

The :math:`outcome` is the prediction value, :math:`\sigma` is the standard deviation.

- "Precision": Positive predictive value disparity ratio.

.. math::

   \begin{align}     
   Precision Ratio = \frac{(TP_{p}) / (TP_{p}+FP_{p})}{(TP_{r}) / (FP_{r}+FN_{r})}
   \end{align}

The value of :math:`TP`, :math:`FP` refers to the count of True Positive samples and False Positive samples, respectively. 
The :math:`p` and :math:`r` refers to the group samples belong to.

- "Recall": True positive rate disparity ratio.

.. math::

   \begin{align}     
   Recall Ratio = \frac{(TP_{p}) / (TP_{p}+FN_{p})}{(TP_{r}) / (TP_{r}+FN_{r})}
   \end{align}

The value of :math:`TP`, :math:`FN` refers to the count of True Positive samples and False negative samples, respectively. And the 'protected' and 'reference' refer to the group samples belong to.

In PIML, you can use model_fairness API to get the metric result, as follows,

.. code-block:: default

	>>> exp.model_fairness(model='XGB2_monotonic', show='performance', metric='AIR', group_category=['Race','Gender'], 
                   reference_group=[1., 1.], protected_group=[0., 0.],
                   favorable_threshold=0.5, performance_metric='ACC')

.. figure:: images/metric.png
   :align: center
   :scale: 60

where `performance` is the keyword of showing metric, and you also need to set the protected group and the referece group.
The output results include:

	- The plot of fairness metric result for each group.
	- The detailed table of fairness metric results for each group.


Fairness Segmented
^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^

PIML also supports showing the segmented fairness result for different metrics.
Given the segmented features and the number of bins, if the feature is categorical,
the samples will be segmented according to the unique categories of the feature.
And if the feature is continuous, the samples will be segmented by the equal space binning method.
After binning, PIML will show the fairness result in each bucket.
The metric types in this section are the same as in the 'Fairness Metric' section.

In PIML, you can use model_fairness API to get the segmented metric result, as follows,

.. code-block:: default

	>>> exp.model_fairness(model='XGB2_monotonic', show='segmented', metric='AIR',
                   segmented_feature='Balance', group_category=['Race','Gender'], 
                   reference_group=[1., 1.], protected_group=[0., 0.],
                   favorable_threshold=0.5, segmented_bins=5)

.. figure:: images/segmented.png
   :align: center
   :scale: 60

where the keyword is `segmented`, the segmented feature is 'Balance' and the segmented_bins is 5,
which means the data is segmented into 5 buckets by feature 'Balance'.
The output results include:

	- The plot of segmented fairness metric results for each group.
	- The detailed table of segmented fairness metric results for each group.


Fairness Thresholding
^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^

For the fairness thresholding function, users can change the classification threshold by inspecting the change of
the fairness and performance result to get a better trade-off result.
This function only works for classification problems.
The fairness metric types in this section are the same as the 'Fairness Metric' section.
The performance metric types:

- "ACC": accuracy value. This value is calculated by the scikit-learn function "accuracy_score", the detail can be found at https://scikit-learn.org/stable/modules/generated/sklearn.metrics.accuracy_score.html.
- "F1": F1 score. This value is calculated by the scikit-learn function "f1_score", the detail can be found at https://scikit-learn.org/stable/modules/generated/sklearn.metrics.f1_score.html.

See the example usage below.

.. code-block:: default

	>>> exp.model_fairness(model='XGB2_monotonic', show='thresholding', metric='AIR', group_category=['Race','Gender'], 
                   reference_group=[1., 1.], protected_group=[0., 0.],
                   favorable_threshold=0.32, performance_metric='ACC')

.. figure:: images/thresholding.png
   :align: center
   :scale: 60

where the keyword here is `thresholding`. 
The 'favorable_threshold' is the classification threshold value, which controls the dashed line position in the plots.
The two y axes is controlled by 'metric' and 'performance_metric', respectively.
The output results include:

- The plot of the fairness metric and performance metric results with a different threshold for each group.
- The detailed table of fairness metric and performance metric results with a different threshold for each group.


Fairness Binning
^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^

PIML supports binning the data to adjust the fairness and performance results.
Different from the 'Fairness Segmented' section, there is not only segmenting the data to show metrics but also changing
the data to get better predictions.
The fairness and Performance metric types in this section are the same as the 'Fairness Thresholding' section.
There are two binning methods, 'quantile' and 'customize'.
Quantile binning aims to assign the same number of observations to each bin, while customize binning only 
works for a specific area by replacing the original value with the mean value of this area.

In PiML, you should first define the binning list, which includes the binning settings you want to compare.
Each binning setting is a key-value pair, and the format is like this:{feature name: {'type': binning type, 'value': binning paramter}}.
If binning type is 'customize', the value is the binning range, while if the type is 'quantile' the value is the number of bins.

.. code-block:: default

	>>> binning_list = [{'Balance':{'type': 'customize', 'value': (0, 0.25)}}]
	>>>	exp.model_fairness(model='XGB2_monotonic', show='binning', metric='AIR', group_category=['Race','Gender'], reference_group=[1., 1.], protected_group=[0., 0.],
						favorable_threshold=0.5, performance_metric='F1', binning_list=binning_list)

.. figure:: images/binning.png
   :align: center
   :scale: 60

where the keyword is `binning`, and the parameter 'binning_list' is for the list of binning settings.
The output results include:

	- The plot of fairness and performance metric results with different binning settings.
	- The detailed table of fairness and performance metric results with different binning settings.


Fairness Feature Selection
^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^

For the fairness selection, In PIML we test both fairness and performance metrics when 
a single feature value is randomly shuffled, to check which feature caused unfairness or bad performance.
The fairness and Performance metric types in this section are the same as the 'Fairness Thresholding' section.

In PIML, you can use model_fairness API to get the feature removal result, as follows,

.. code-block:: default

	>>> exp.model_fairness(model='XGB2_monotonic', show='feature_removal', metric='AIR', group_category=['Race','Gender'], 
                   reference_group=[1., 1.], protected_group=[0., 0.],
                   favorable_threshold=0.5, performance_metric='F1',)

.. figure:: images/feature_select.png
   :align: center
   :scale: 60

where the keyword here is `feature_removal`. Other parameters are similar to the above method.
The output results include:

- The plot of fairness and performance metric results with different features shuffled.
- The detailed table of fairness and performance metric results with different features shuffled.

Full Example
^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^

The full example codes of this section can be found in the following link.

.. topic:: Example

	* :ref:`sphx_glr_auto_examples_diagnostics_plot_1_fairness.py`

