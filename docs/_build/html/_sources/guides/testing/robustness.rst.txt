
.. Places parent toc into the sidebar

:parenttoc: True

.. include:: ../../includes/big_toc_css.rst

============================
Robustness
============================
The performance of a model may significantly degrade when it encounters noisy data or distribution shifts, leading to incorrect predictions. Such data drift or shift may arise due to unexpected changes, and then alter the underlying patterns and relationships between the input and target variables. In this section, we will showcase how to leverage PiML to assess the model's robustness to input perturbations.


Algorithm Details 
^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
This section briefly describes key perturbation approaches used in PiML to simulate the data distribution shift. Perturbed input will be used for robustness evaluation. 

Perturbation For Numerical Features
""""""""""""""""""""""""""""""""""""""""
For numerical features, we provide the following two perturbation options. 

**Raw perturbation**
We added random Gaussian noise with mean 0 and variance :math:`\lambda^2` Var(:math:`x`) to the input :math:`x`.  :math:`\lambda` is the perturbation size (`perturb_size`) used to vary the variance, hence the noise level. 

**Quantile Perturbation**
Assume a discrete variable :math:`x`. We sort :math:`x` and compute the quantile (:math:`Q`) of each element of :math:`x`, leading to the following table.

.. list-table:: 
   :widths: 55   35  35  35  35  35  35  35  35  35  35 
   :header-rows: 1

   * - :math:`x`
     - 1
     - 2
     - 2
     - 2
     - 3
     - 3 
     - 3
     - 40
     - 40
     - 50
   * - :math:`Quantile`
     - 0.1
     - 0.2
     - 0.3
     - 0.4
     - 0.5
     - 0.6
     - 0.7
     - 0.8
     - 0.9
     - 1.0

Consider the data point with value 3, with a corresponding quantile value of 0.6. Three (3) can be perturbed to 40 as below. Perform perturbation in quantile scale, which consists of adding a small noise to each quantile the round to the nearest available value. For instance, in the case of 3, let's say we perturb 0.6 and get 0.86. We round 0.86 to 0.9, which is the quantile of 40 (according to the table above). Therefore, we perturbed 3 with 40.

Perturbation for Categorical Variable 
""""""""""""""""""""""""""""""""""""""""
For illustration, let's assume we have three categories (A, B, and C). The user must specify the perturbation probability :math:`p` (`perturb_size`) and the transformation matrix as represented below. 

.. list-table:: 
   :widths: 80 70 70 70 
   :header-rows: 1

   * - 
     - A
     - B
     - C
   * - **A** 
     - 0.3
     - 0.3
     - 0.4
   * - **B**
     - 0.2
     - 0.5
     - 0.3
   * - **C**
     - 0.4
     - 0.3
     - 0.4

To perturb, select samples with probability :math:`p` (`perturb_size`) and keep them unchanged with probability :math:`1-p`. :math:`A` will be perturbed to :math:`A`, :math:`B` and :math:`C`  with probability 30%, 30%, and 40%, respectively.   


Usage 
^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
Robustness test can be used by the `model_diagnose` function, of which two options are provided, i.e., "robustness_perf" and "robustness_perf_worst". The former is used to evaluate the model's robustness under perturbation, while the latter is used to evaluate the model's performance against perturbation size based on the worst sample. The following arguments are required to run the robustness test.

- `perturb_method`: The perturbation method, by default "raw", which means adding normal noise to the covariates for perturbation. The alternative option is "quantile", which means first transforming the data to a quantile scale, then adding uniform noise to the quantiles, and finally inversely transforming the perturbed data to the raw scale.
- `perturb_features`: The features to be perturbed, by default None, which means all features will be perturbed.
- `perturb_size`: The perturbation step.
- `metric`: The performance metric to be used. Supported metrics are "ACC", "AUC", and "F1" for binary classification; "MSE", "MAE", and "R2" for regression.
- `alpha`: The proportion of worst samples to consider. 

For illustration, we consider a FIGS model on the BikeSharing data. 

Robustness on the whole test sample
"""""""""""""""""""""""""""""""""""""""""""""""""""""""""
To assess the models' performance against noise, we set the parameter `show` to "robustness_perf". By default, all the features will be perturbed, and you may choose the set of features to perturb as specified in the examples below.

.. jupyter-input::

    exp.model_diagnose(model="FIGS", show='robustness_perf', perturb_features=None,
                      perturb_method="raw", metric="MSE", perturb_size=0.1, figsize=(6, 4))

.. figure:: ../../auto_examples/testing/images/sphx_glr_plot_4_robustness_reg_001.png
   :target: ../../auto_examples/testing/plot_4_robustness_reg.html
   :align: left

In the figure above, the x-axis represents the noise level and the y-axis shows the test set performance upon perturbation. The box plot displays the results over ten repetitions. That is, we add different random noises for each repetition and evaluate and record the metrics. The noise level of 0.0 indicates no perturbation, and hence there is no variation in the performance. The full plot above encapsulate the model performance at each noise level. As expected, we observe a degradation in model performance as the noise level increases.

The second example perturbs two features `hr` and `atemp`, while the rest settings remain the same as the first example, as shown below. As less number of features are perturbed, the performance degradation looks smaller than in the first example.

.. jupyter-input::

    exp.model_diagnose(model="FIGS", show="robustness_perf", perturb_features=["hr", "atemp"],
                      perturb_method='raw', metric="MSE", perturb_size=0.1,  figsize=(6, 4))

.. figure:: ../../auto_examples/testing/images/sphx_glr_plot_4_robustness_reg_002.png
   :target: ../../auto_examples/testing/plot_4_robustness_reg.html
   :align: left

In summary, the box plots can be used to assess model robustness under different input perturbation settings, as illustrated in the examples above. The less drop in model performance under input perturbation, the more robust a robust model. This observation also applies to the worst-performing samples discussed in the following section. 

Robustness on worst test samples
""""""""""""""""""""""""""""""""""
This section illustrates how to evaluate model robustness under the worst-performing test samples. In this context, we identify the :math:`30\%` (`alpha=0.3`) worst test samples, i.e., the 30% test samples with the largest absolute residuals), then apply the perturbations to these worst samples. 

.. jupyter-input::
   
    exp.model_diagnose(model="FIGS", show="robustness_perf_worst", perturb_features=None,
                      perturb_method="raw", metric="MSE", perturb_size=0.1, alpha=0.3, figsize=(6, 4))

.. figure:: ../../auto_examples/testing/images/sphx_glr_plot_4_robustness_reg_006.png
   :target: ../../auto_examples/testing/plot_4_robustness_reg.html
   :align: left

On the x-axis, we have the noise level (perturbation size), and on the y-axis, the model performance (MSE). When evaluating under worst sample perturbation, we observe that the performance is much worse than the full sample. This is within expectation since we only deal with worst-performing samples in this context. The drop in model performance increase with the noise level. This plot tells us how poorly the model will perform under the input perturbation of the worst-performing samples.


Examples
^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^

.. topic:: Example 1: BikeSharing

  The first example below demonstrates how to use PiML with its high-code APIs for developing machine learning models for the BikeSharing data from the UCI repository, which consists of 17,389 samples of hourly counts of rental bikes in Capital bikeshare system; see details. The response `cnt` (hourly bike rental counts) is continuous and it is a regression problem.
 
 * :ref:`sphx_glr_auto_examples_testing_plot_4_robustness_reg.py`

.. topic:: Examples 2: Taiwan Credit

  The second example below demonstrates how to use PiMLâ€™s high-code APIs for the TaiwanCredit dataset from the UCI repository. This dataset comprises the credit card details of 30,000 clients in Taiwan from April 2005 to September 2005, and more information can be found on the TaiwanCreditData website. The data can be loaded directly into PiML, although it requires some preprocessing. The FlagDefault variable serves as the response for this classification problem.
    
 * :ref:`sphx_glr_auto_examples_testing_plot_4_robustness_cls.py`
