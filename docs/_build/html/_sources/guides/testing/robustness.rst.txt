
.. Places parent toc into the sidebar

:parenttoc: True

.. include:: ../includes/big_toc_css.rst

============================
Model Robustness
============================

Widely used in image recognition, natural language processing, self-driving car, and fraud detection, to name a few, the progress of deep learning is astounding! We often rely on performance metrics such as MSE, MAE, ACC, AUC, Recall, Precision, 
and F1-score to evaluate our model performance. However, researchers have demonstrated that machine learning models are vulnerable to adversarial examples/noisy data, which may cause the model to predict with high
confidence a wrong class [A2018]_ . In the adversary context, the attacker aims to induce the model (Artificial intelligence system) to return incorrect outputs advantageous for the attacker by adding a carefully imperceivable noise to the input. 
If successful, such behavior may lead to unexpected financial losses (Credit, Market, Liquidity) or Non-financial harms (Reputation, Compliance, Legal). On the other hand, random noise is a common cause of most of the data shift/drift that occurs 
in real life and may originate from an unexpected change such as the change in consumer behaviors, worlds conflicts, technology breakthroughs, pandemics, socioeconomic and other unpredictable factors that can
dramatically change the input data, the target or the underlying patterns and relations between input and the target variable. We must evaluate all models under distribution shift.

Robustness and Regularization
------------------------------------

Let :math:`l()` represents the loss function, :math:`f_{\theta}()` the model with parameters :math:`\theta` (weigths and bias). Training :math:`f_{\theta}()` under regularization consists of minimizing the loss
:math:`l(f_{\theta}(x),y)+ \lambda  \phi (w)` where the regularization term :math:`\phi` is used to prevent over-fitting. Hence, controlling the bias and variance of the model. A too-limited model may not capture all the information encapsulated
in the data. On the other hand, if the model is too complex, it will capture unnecessary information (noise). In both cases, the model will perform poorly on new data. Sietsma [S1991]_ experimentally demonstrated that training with noise could
improve model robustness, which is also applicable in the context of adversarial robustness, cofirmend by Goodfellow [G2014]_ with adversarial training ( training with adversarial examples). Much later, Bishop [B1995]_ established the connection
between training with noise and regularization and showed that training with noise is equivalent to training with the sum-of-squares error (:math:`\phi (w) = L_{2} (w)`) as regularization term. The :math:`L_{2}` penalty robustifies the model but 
fails to simplify the model. On the other hand, the :math:`L_{1}` constraint the model to learn small weights or regularize towards zero, leading to a much simpler model, hence, not over-parameterized.

Data shift (Input Perturbation)
-----------------------------------
This section briefly describes key perturbation approaches used in PiML to simulate the data distribution shift. Perturbed input will be used for model robustness evaluation. 

- **Perturbation of continuous variables**: We added random Gaussian noise with mean 0 and variance :math:`λ(variance(X))` to the input :math:`X` to evaluate for robustness, of which :math:`λ`(`perturb_size`) is used to 
  vary the variance, hence the noise level.
- **Discrete Variable Perturbation**: The perturbation of discrete random variables is much more complex. To facilitate our understanding, consider a sorted discrete variable :math:`X` with values :math:`[1, 2, 2, 3, 30, 50,50, 50]` and 
  corresponding Quantiles vector :math:`Q: [0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8]`. If we perturb :math:`Q` with a small random noise :math:`\epsilon` such that :math:`\epsilon` follows the uniform distribution 
  (:math:`U(-0.5*perturb \_ step*noise \_ level, 0.5*perturb \_ step*noise \_ level)`). The user may adjust the perturbation step (`perturb_step`) and the `noise_level`, which can take the values 1,2,3, or 4 corresponding to the 
  four noise levels. For instance, in :math:`Q`, if the perturbed value of 0.3 (corresponding to 2 in :math:`X`) is 0.36, we round 0.36 to 0.4 (which corresponds to 3 in :math:`X`), then we conclude that the perturbed value of 2 is 3.
- **Categorical Variable Perturbation**: We perturb a sample with probability :math:`p` (`perturb_size`) by standard deviation of categorical feature.
 
Robustness Test 
------------------------------------

Binary classification tasks 
^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^

We consider a binary classification task on the  TaiwanCredit data from UCI repository. The response to this classification problem is 'FlagDefault'.
Let's start by training four separate models; we then demonstrate how to use PiML to asses individual robustness or compare models in terms of robustness (this approach can be extended to
multiple models). It is important to note that PiMl only focuses on robustness to random noise, not adversarial robustness.

.. _TaiwanCreditData: https://archive.ics.uci.edu/ml/datasets/default+of+credit+card+clients

Moodels instantiation and training
"""""""""""""""""""""""""""""""""""""""""""""""

.. jupyter-input::

        import ipywidgets
        with ipywidgets.Output():
          from piml import Experiment
          from piml.models import FIGSClassifier
          from piml.models import ReluDNNClassifier
          from piml.models import GAMINetClassifier
          exp = Experiment()
          exp.data_loader(data='TaiwanCredit')
          exp.data_summary(feature_exclude=["LIMIT_BAL", "SEX", "EDUCATION", "MARRIAGE", "AGE"], feature_type={})
          exp.data_prepare(target='FlagDefault', task_type='Classification', test_ratio=0.2, random_state=0)
        
.. jupyter-input::
    
     exp.model_train(FIGSClassifier(max_iter=200, max_depth=None, splitter='best', min_samples_leaf=1,
                                    min_impurity_decrease=0, learning_rate=0.0002, 
                                    random_state=None),name="FIGS")
     exp.model_train(ReluDNNClassifier(hidden_layer_sizes=(60,60), l1_reg=0.0002, batch_size=200,
                                       learning_rate=0.0002),name="ReluDNN_A")
     exp.model_train(ReluDNNClassifier(hidden_layer_sizes=(60,60), l1_reg=0.003, batch_size=200,
                                       learning_rate=0.0002),name="ReluDNN_B")
     exp.model_train(GAMINetClassifier(interact_num=10, loss_threshold=0.01,
                                       subnet_size_main_effect=[20],
                                       subnet_size_interaction=[20,20]),name='GAMINet')

Model performance on natural data (No perturbation appied) 
"""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""

.. jupyter-input::
   
      exp.model_compare(models=["FIGS","ReluDNN_A","ReluDNN_B","GAMINet"], 
                        show='accuracy_auc', figsize=(5,4))

.. figure:: ../../auto_examples/diagnostics/images/sphx_glr_plot_3_robustness_001.png
   :target: ../../auto_examples/diagnostics/plot_3_robustness_001.html                            
   :align: left
   :scale: 80%

Considering AUC as our evaluation criteria, two models (ReluDNN_A and GAMINet) recorded a comparable performance. However, are these models truly comparable? In the following section, we demonstrated how to use PiML to 
assess the robustness of these models under different noise levels. Assessing models under unexpected changes is strongly recommended to minimize the risks after deployment.

Model Robustness
"""""""""""""""""""""""""

We first consider the performance of individual models. In addition, we can also compare the performance of many models at the same time (in this demo, we consider four models). The 
robustness assessment helps identify a more robust and suitable model for a particular task.

PiML offers the flexibility to choose a perturbation step size. The plot below is under the default step size (0.1).

.. jupyter-input::
   
      exp.model_diagnose(model="ReluDNN_A", show='robustness', 
                         perturb_features='All Features', alpha=0.3)

.. figure:: ../../auto_examples/diagnostics/images/sphx_glr_plot_3_robustness_002.png
   :target: ../../auto_examples/diagnostics/plot_3_robustness_002.html
   :align: left
   :scale: 40% 
   
.. figure:: ../../auto_examples/diagnostics/images/sphx_glr_plot_3_robustness_003.png
   :target: ../../auto_examples/diagnostics/plot_3_robustness_003.html
   :align: left
   :scale: 40% 

The plot on the right represents the model performance when we perturb the input. On the other hand, the left plot  has the models' performance evaluated on the :math:`30\%` worst samples.  At noise level 0.0, no perturbation is applied.  
The worst sample AUC is approximately 0.315, and the models' performance on the testing data is 0.793. Under perturbation, the worst samples recorded a significant drop in AUC. The worst sample scenario may help asses the model under the 
worst-case scenario. In the following plot, we consider the step size of 0.01.

.. jupyter-input::
                                    
      exp.model_diagnose(model="ReluDNN_A", show='robustness',
                         perturb_size=0.01,
                         perturb_features='All Features', alpha=0.3)

.. figure:: ../../auto_examples/diagnostics/images/sphx_glr_plot_3_robustness_004.png
   :target: ../../auto_examples/diagnostics/plot_3_robustness_004.html
   :align: left
   :scale: 40%

.. figure:: ../../auto_examples/diagnostics/images/sphx_glr_plot_3_robustness_005.png
   :target: ../../auto_examples/diagnostics/plot_3_robustness_005.html
   :align: left                     
   :scale: 40%  

We should be cautious about selecting a model for a particular task. In the following, we demonstrate how to use PiML to compare the robustness performance of different models. We consider comparing
completely different types of models and similar model architectures but trained under different parameters. The sample with the low AUC is consider the worst sample. We can use PiML to evaluate the model
performance on the selected worst samples by setting the parameter `show` to `robustness_perf_worst`. The default perturbation method in the robustness test is ‘raw’, which consists of adding normal noise directly on covariates for perturbation 
(Noise applied on continuous variables).

.. jupyter-input::
   
      exp.model_compare(models=["FIGS","ReluDNN_A", "ReluDNN_B", "GAMINet"], 
                        show='robustness_perf_worst', alpha=0.3, figsize=(5,4))

.. figure:: ../../auto_examples/diagnostics/images/sphx_glr_plot_3_robustness_006.png
   :target: ../../auto_examples/diagnostics/plot_3_robustness_006.html
   :align: left
   :scale: 80%

The plot above compares models under the worst-performing test samples. In this context, we identify the :math:`30\%` worst performing test sample (Low performing test samples are test samples with low AUC), then apply a perturbation. Under 
perturbation (distribution drift), the plot above indicates that model `ReluDNN_B` will perform better on the worst sample. The worst-performing model is `FIGS`. We may not only compare the Robustness performance under the worst-case scenario. 
For general robustness, set `show` to `robustness_perf` as follows.

.. jupyter-input::

      exp.model_compare(models=["FIGS","ReluDNN_A","ReluDNN_B","GAMINet"], 
                        show='robustness_perf', figsize=(5,4))

.. figure:: ../../auto_examples/diagnostics/images/sphx_glr_plot_3_robustness_007.png
   :target: ../../auto_examples/diagnostics/plot_3_robustness_007.html
   :align: left
   :scale: 80%

To perturb discrete variables, use the ‘quantile’ perturbation method, as illustrated below.   

.. jupyter-input::
      
      exp.model_compare(models=["FIGS","ReluDNN_A","ReluDNN_B","GAMINet"],
                        perturb_method = 'quantile',
                        show='robustness_perf', figsize=(5,4))

.. figure:: ../../auto_examples/diagnostics/images/sphx_glr_plot_3_robustness_008.png
   :target: ../../auto_examples/diagnostics/plot_3_robustness_008.html
   :align: left
   :scale: 80%

The plot above compares models under different noise levels. In this case, the perturbation is applied to all the variables. Cleary, models `ReluDNN_A` and `GAMINet` are not 
comparable. Hence, contradicted the observation made under natural evaluation.  Model `ReluDNN_A` recorded a better robust performance, followed by `ReluDNN_B`. In addition,  the plot above indicates that we should 
always consider different parameter settings as they can influence the model's robustness. For instance, models `ReluDNN_A` and `ReluDNN_B` recorded different robust performances. 
However, they share the same architecture but are trained under different regularization strengths.

Regression tasks.
^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^

For a regression task, refer to the example 2.

Examples
^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
   
.. topic:: Example 1
   
 The example below demonstrates how to use PiML with its high-code APIs for developing machine learning models for the BikeSharing data from UCI repository, which consists of 17,389
 samples of hourly counts of rental bikes in Capital bikeshare system; see details `here`_. The response `cnt` (hourly bike rental counts) is continuous and it is a regression problem.
  
 .. _here: https://archive.ics.uci.edu/ml/datasets/bike+sharing+dataset
 * :ref:`sphx_glr_auto_examples_diagnostics_plot_3_robustness.py`

.. topic:: Example 2
 
 We consider a binary classification task on the  TaiwanCredit data from UCI repository, which consists of 30,000 clients' credit cards in Taiwan from 200504 to 200509; see details here
 `TaiwanCreditData`_ . The data can be loaded from PiML and subject to slight preprocessing. The response to this classification problem is 'FlagDefault'.

 * :ref:`sphx_glr_auto_examples_diagnostics_plot_2_robustness.py`
   
.. topic:: References
     
     .. [A2018] Athanasios Voulodimos, Nikolaos Doulamis, Anastasios Doulamis, and Eftychios Protopapadakis.
               `Deep learning for computer vision: A brief review. 
               <Deep Learning for Computer Vision: A Brief Review (hindawi.com)>`_,
               Computational intelligence and neuroscience, vol. 2018, 2018.

     .. [S1991] Sietsma, Jocelyn, and Robert JF Dow.
               `Creating artificial neural networks that generalize.
               <https://www.sciencedirect.com/science/article/abs/pii/0893608091900332>`_,
               Neural networks 4.1 (1991): 67-79.

     .. [G2014] Goodfellow, Ian J., Jonathon Shlens, and Christian Szegedy.
               `Explaining and harnessing adversarial examples.
               <https://arxiv.org/abs/1412.6572>`_,
               arXiv preprint arXiv:1412.6572 (2014).
      
     .. [B1995] Bishop, Chris M.                                 
               `Training with noise is equivalent to Tikhonov regularization.   
               <https://ieeexplore.ieee.org/abstract/document/6796505>`_,
               Neural computation 7.1 (1995): 108-116.


