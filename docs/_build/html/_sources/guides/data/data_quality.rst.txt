.. Places parent toc into the sidebar

:parenttoc: True

.. include:: ../../includes/big_toc_css.rst

======================================
Data Quality
======================================

In PiML, we focus only on data outliers. We support four different outlier detection methods and analysis results to help users locate and remove the outliers: Isolation Forest, Cluster-Based Local Outlier Factor, Principal Component Analysis and Kmeans Tree.

Method
^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^

We support four outlier detection methods, Isolation Forest, Cluster-Based Local Outlier Factor, Principal Component Analysis and Kmeans Tree.


Isolation Forest
""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""
The Isolation Forest 'isolates' observations by randomly selecting a feature and then randomly selecting a split value between the maximum and minimum values of the selected feature. The rationale is as follows.

Since recursive partitioning can be represented by a tree structure, the number of splits required to isolate an observation  equals the path length from the root node to the terminal node. This path length, averaged over a forest of such random trees, is a measure of the “normalness” of the data point. For outliers, random partitioning produces noticeably shorter paths. Hence, when a forest of random trees collectively produces shorter path lengths for  particular data points, they are highly likely to be anomalies.

This `IsolationForest`_ class in PiML is a warpper of `sklearn.ensemble.IsolationForest`_. It is based on an ensemble of trees.ExtraTreeRegressor. And the maximum depth of each tree is set to less than :math:`log_{2}(n)` where :math:`n` is the number of samples used to build the tree (see [Liu2008]_ for more details).

.. _sklearn.ensemble.IsolationForest: https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.IsolationForest.html

.. _IsolationForest: ../../modules/generated/piml.data.outlier_detection.IsolationForest.html


Cluster-Based Local Outlier Factor (CBLOF)
""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""
This is an outlier detection method based on clusters proposed by [He2003]_. The method can be broken down into several steps:

    1.	Clustering: Divide the dataset into clusters using a clustering algorithm such as k-means or DBSCAN.
    2.	Cluster sizes: For each cluster, compute the size of the cluster (number of points).
    3.	Intra distance: If the cluster size is larger than the (given) threshold, the scores of samples in this cluster are the distance between them and their cluster centroid.
    4.	Inter distance: If the cluster size is smaller than the (given) threshold, the scores of samples in this cluster are the distance between them and the nearest cluster centroid.
    5.	CBLOF score: The resulting distance multiplied by the cluster size.

In this method CBLOF_, we have several hyperparameters, for instance:

- `use_weights`: If false, the outlier score will not multiply the cluster size to calculate CBLOF.

- `clustering_method:`: We support 'Kmeans' and 'Gaussian Mixture Model' for getting the initial clusters.

.. _CBLOF: ../../modules/generated/piml.data.outlier_detection.CBLOF.html


Principal Component Analysis
""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""
PCA or other dimensionality reduction methods can also be used to detect outliers. In PiML, we support two different methods to calculate the outlier score, Mahalanobis distance and error reconstruction, see more details in PCA_.

- **Mahalanobis distance**: The Mahalanobis distance is equal to the sum of squares of the scores of all non-zero standardized principal components. The Mahalanobis distance can be got easily with PCA under the formula.

.. math::
    MD(x)^2 = \sum z_{i}^{2} / \lambda_{i}

where :math:`z_{i}` are the :math:`i`-th principal component scores, and :math:`\lambda_{i}` is the :math:`i`-th eigenvalue of the covariance matrix which can be explained as the variance. But this is precisely the sum of squared distances in the transformed PCA space, which gives us the desired result.

- **Error reconstruction**: Fit a XGBoost model between the principal components and the covariates. This model is then used to reconstruct the covariates :math:`X_{new}`. The difference between the original covariates and the reconstructed data is calculated as the reconstruction error, i.e., :math:`X - X_{new}`. Finally, we calculate the Mahalanobis distance of the reconstruction error as the final outlier score. If the reconstruction errors of each feature are mutually independent, the outlier score reduces to mean squared reconstruction error.

.. _PCA: ../../modules/generated/piml.data.outlier_detection.PCA.html


KmeansTree
"""""""""""""""""""""""""""""""""
To take advantage of the cluster-based method and PCA-based method, we proposed a new outlier detection method called KmeansTree. The algorithm is below:

    1.	Use the K-means cluster method with K equal to 2 to split the dataset iteratively.
    2.	Stop splitting until certain conditions are met (max depth, max leves, distributional distance…).
    3.	For samples in each cluster, calculate the principal components using PCA.
    4.	Fit an XGB model between the principal components and the original predictors, and calculate the reconstruction error.
    5.	Calculate the Mahalanobis distance of the reconstruction error, and use it as the outlier score.

Because the condition we use to stop splitting and the splitting way is similar to the decision tree, we call this method KmeansTree. As PCA can only handle linear relationships and is sensitive to the outlier, the homogeneity of the data is increased after clustering which could improve the outlier detection performance, see more details in KMeansTree_.

.. _KMeansTree: ../../modules/generated/piml.data.outlier_detection.KMeansTree.html


Analysis and Comparison
^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
After getting the scores of each outlier detection method, we need to choose a proper threshold to define whether a sample is an outlier. We also need to compare different methods to decide which to use. PiML supports two functions to visualize the outlier detection result, a score distribution plot and marginal outlier distribution plot and one t-SNE plot to compare results of different methods.

Here is an example of displaying the score distribution of the PCA-based outlier detection method.

.. jupyter-input::

    from piml.data.outlier_detection import PCA
    exp.data_quality_check(method=PCA(), show='score_distribution', threshold=0.999)

.. figure:: ../../auto_examples/data/images/sphx_glr_plot_4_data_quality_001.png
   :target: ../../auto_examples/data/plot_4_data_quality.html
   :align: left

Here is an example of displaying the marginal distribution of detected outliers.

.. jupyter-input::

    from piml.data.outlier_detection import PCA
    exp.data_quality_check(method=PCA(), show='marginal_outlier_distribution', threshold=0.999)

.. figure:: ../../auto_examples/data/images/sphx_glr_plot_4_data_quality_002.png
   :target: ../../auto_examples/data/plot_4_data_quality.html
   :align: left

We use t-SNE to reduce the dimension of data to 2D for better visualization.
Here is an example of comparing different methods:

.. jupyter-input::

    from piml.data.outlier_detection import PCA, CBLOF
    exp.data_quality_check(method=[PCA(), CBLOF()], show='tsne_comparison', threshold=[0.999, 0.999])

.. figure:: ../../auto_examples/data/images/sphx_glr_plot_4_data_quality_003.png
   :target: ../../auto_examples/data/plot_4_data_quality.html
   :align: left



Examples
^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^

The full example codes of this section can be found in the following link.

.. topic:: Example

	* :ref:`sphx_glr_auto_examples_data_plot_4_data_quality.py`


.. topic:: References

    .. [Liu2008] Fei Tony Liu, Kai Ming Ting, Zhi-Hua Zhou (2008). `Isolation Forest <https://ieeexplore.ieee.org/abstract/document/4781136>`_, 2008 Eighth IEEE International Conference on Data Mining, Pisa, Italy, 2008, pp. 413-422, doi: 10.1109/ICDM.2008.17.

    .. [He2003] Zengyou He, Xiaofei Xu, Shengchun Deng (2003). `Discovering cluster-based local outliers <https://www.sciencedirect.com/science/article/abs/pii/S0167865503000035>`_, Pattern recognition letters, 24(9-10), 1641-1650.
