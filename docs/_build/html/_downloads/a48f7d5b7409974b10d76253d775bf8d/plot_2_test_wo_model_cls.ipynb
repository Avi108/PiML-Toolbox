{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "%matplotlib inline"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n# Scored Test (Classification)\nTesting External Models without Model Objects as Input\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Assume we have models fitted outside PiML workflow\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "import numpy as np\nfrom xgboost import XGBClassifier\nfrom sklearn.datasets import make_circles\nfrom sklearn.model_selection import train_test_split\n\nx, y = make_circles(n_samples=2000, noise=0.1, random_state=0)\ntrain_x, test_x, train_y, test_y, train_idx, test_idx = train_test_split(x, y,\n                                                                         np.arange(x.shape[0]), test_size=0.2)\n\nxgb2 = XGBClassifier(max_depth=2, n_estimators=100)\nxgb2.fit(train_x, train_y)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Prepare the data for testing \n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "random_state = 0\ntarget_name = \"target\"\ntask_type = 'classification'\nfeature_names = [\"X1\", \"X2\"]\nfeature_types = [\"numerical\", \"numerical\"] # \"categorical\" or \"numerical\"\nprediction = xgb2.predict(x)\nprediction_proba = xgb2.predict_proba(x)[:, 1]\n\ndata_params = {'x': x,\n               'y': y,\n               'prediction': prediction,\n               'prediction_proba': prediction_proba,\n               'feature_names': feature_names,\n               'feature_types': feature_types,\n               'target_name': target_name,\n               'train_idx': train_idx,\n               'test_idx': test_idx,\n               'task_type': task_type,\n               'random_state': random_state}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Show the accuracy table \n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "from piml.scored_test import test_accuracy_table\nres = test_accuracy_table(**data_params)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Plot confusion matrix, ROC and Recall-Precision, only supports classifiers\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "from piml.scored_test import test_accuracy_plot\nres = test_accuracy_plot(**data_params, figsize=(10, 4))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Plot the prediction residuals against one feature of interest \n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "from piml.scored_test import test_accuracy_residual\nres = test_accuracy_residual(**data_params, show_feature='X1', figsize=(6, 5))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Run weakspot test to detect weak regions\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "from piml.scored_test import test_weakspot\nres = test_weakspot(**data_params, slice_features=['X1'], figsize=(6, 5))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Run overfit test to detect overfit regions\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "from piml.scored_test import test_overfit\nres = test_overfit(**data_params, slice_features=['X1'], figsize=(6, 5))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Run reliability diagram \n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "from piml.scored_test import test_reliability_perf\nres = test_reliability_perf(**data_params, alpha=0.1, bins=10, figsize=(6, 5))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Run reliability test to show the distributional distance between reliable and unreliable samples. \n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "from piml.scored_test import test_reliability_calibration\nres = test_reliability_calibration(**data_params, alpha=0.1, figsize=(6, 5))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Run reliability test to show the average coverage of prediction intervals \n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "from piml.scored_test import test_reliability_table\nres = test_reliability_table(**data_params, alpha=0.1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Run reliability test to show the distributional distance between reliable and unreliable samples. \n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "from piml.scored_test import test_reliability_distance\nres = test_reliability_distance(**data_params, alpha=0.1, threshold=1.1, figsize=(6, 5))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Run reliability test to show relationship between prediction interval width and the feature of interest \n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "from piml.scored_test import test_reliability_marginal\nres = test_reliability_marginal(**data_params, alpha=0.1, threshold=1.1, show_feature='X1',\n                                bins=10, figsize=(6, 5))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Run resilience test to show how model performance changes under distributional shift.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "from piml.scored_test import test_resilience_perf\nres = test_resilience_perf(**data_params, figsize=(6, 5))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Run resilience test to show the distributional distance between worst samples and remaining samples.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "from piml.scored_test import test_resilience_distance\nres = test_resilience_distance(**data_params, figsize=(6, 5))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Run resilience test to show how model performance changes under distributional shift (density plot).\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "from piml.scored_test import test_resilience_shift_density\nres = test_resilience_shift_density(**data_params, alpha=0.3, metric='AUC', show_feature='X1',\n                                    figsize=(6, 5))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Run resilience test to show how model performance changes under distributional shift (histogram plot).\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "from piml.scored_test import test_resilience_shift_histogram\nres = test_resilience_shift_histogram(**data_params, alpha=0.3, metric='AUC', show_feature='X1',\n                                      figsize=(6, 5))"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}